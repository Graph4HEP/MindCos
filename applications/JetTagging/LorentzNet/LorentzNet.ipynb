{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce27bda0-2fd9-4b01-b866-7597df3f1d82",
   "metadata": {},
   "source": [
    "# LorentzNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14fdcc2d-dd7c-47ec-9407-1ce874979361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary packages\n",
    "import numpy as np\n",
    "import energyflow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#mindspore packages\n",
    "import mindspore.dataset as mds\n",
    "import mindspore.dataset.transforms.c_transforms as C\n",
    "import mindspore as ms\n",
    "from mindspore import Tensor, ops\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import XavierUniform\n",
    "from mindspore import context\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dbb8ef-c890-48e0-9a69-709514af8bec",
   "metadata": {},
   "source": [
    "define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d2f4e03-5994-429a-896b-8eda800c5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 MindSpore 的数据集类\n",
    "class JetDataset(mds.Dataset):\n",
    "    def __init__(self, label, p4s, nodes, atom_mask, batch_size, repeat_size=1, num_parallel_workers=1):\n",
    "        self.label = label\n",
    "        self.p4s = p4s\n",
    "        self.nodes = nodes\n",
    "        self.atom_mask = atom_mask\n",
    "        self.batch_size = batch_size\n",
    "        self.repeat_size = repeat_size\n",
    "        self.num_parallel_workers = num_parallel_workers\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取单个样本\n",
    "        return (self.label[idx], self.p4s[idx], self.nodes[idx], self.atom_mask[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        # 数据集大小\n",
    "        return len(self.label)\n",
    "\n",
    "    def build(self, column_names=None):\n",
    "        # 构建数据集\n",
    "        ds = mds.NumpySlicesDataset((self.label, self.p4s, self.nodes, self.atom_mask), column_names=['label', 'p4s', 'nodes', 'atom_mask'], sampler=mds.RandomSampler())\n",
    "        # 设置 batch 大小和重复次数\n",
    "        ds = ds.batch(self.batch_size, drop_remainder=True).repeat(self.repeat_size)        \n",
    "        return ds\n",
    "\n",
    "    def map(self):\n",
    "        # 设置并行工作数\n",
    "        ds = self.build()\n",
    "        ds = ds.map(operations=self.collate_fn, input_columns=['label', 'p4s', 'nodes', 'atom_mask'], output_columns=['label', 'p4s', 'nodes', 'atom_mask', 'edge_mask', 'edges'], \n",
    "                    num_parallel_workers=self.num_parallel_workers)\n",
    "        return ds\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(label, p4s, nodes, atom_mask):\n",
    "        # 定义 collate_fn 函数，与 PyTorch中的 collate_fn 类似  \n",
    "        batch_size = p4s.shape[0]\n",
    "        n_nodes = p4s.shape[1]\n",
    "        edge_mask = np.expand_dims(atom_mask, axis=1) * np.expand_dims(atom_mask, axis=2)\n",
    "        diag_mask = np.eye(edge_mask.shape[1], dtype=bool)\n",
    "        diag_mask = ~np.expand_dims(diag_mask, axis=0)\n",
    "        edge_mask *= diag_mask\n",
    "        edges = JetDataset.get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "        return label, p4s, nodes, atom_mask, edge_mask, edges\n",
    "\n",
    "    @staticmethod\n",
    "    def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "        # 定义 get_adj_matrix 函数，与 PyTorch 中的 get_adj_matrix 类似\n",
    "        rows, cols = [], []\n",
    "        for batch_idx in range(batch_size):\n",
    "            nn = batch_idx * n_nodes\n",
    "            x = edge_mask[batch_idx]\n",
    "            rows.append(nn + np.where(x)[0])\n",
    "            cols.append(nn + np.where(x)[1])\n",
    "        rows = np.concatenate(rows)\n",
    "        cols = np.concatenate(cols)\n",
    "        return rows, cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303b8d5-f4bf-45ff-9cf2-7345982c3282",
   "metadata": {},
   "source": [
    "define the dataloader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62922d2-1eb5-4b42-95d3-01e30f0f3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataloaders(batch_size, num_data=-1, use_one_hot=True, cache_dir='./data'):\n",
    "    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n",
    "                                  with_bc=False, cache_dir=cache_dir)\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = {type: {'raw': None, 'label': None} for type in splits}\n",
    "    (data['train']['raw'], data['val']['raw'], data['test']['raw'],\n",
    "     data['train']['label'], data['val']['label'], data['test']['label']) = \\\n",
    "        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle=False)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore').fit([[11], [13], [22], [130], [211], [321], [2112], [2212]])\n",
    "\n",
    "    for split, value in data.items():\n",
    "        pid = np.abs(np.asarray(value['raw'][..., 3], dtype=int))[..., None]\n",
    "        p4s = energyflow.p4s_from_ptyphipids(value['raw'], error_on_unknown=True)\n",
    "        one_hot = enc.transform(pid.reshape(-1, 1)).toarray().reshape(pid.shape[:2] + (-1,))\n",
    "        one_hot = np.array(one_hot)\n",
    "        mass = energyflow.ms_from_p4s(p4s)[..., None]\n",
    "        charge = energyflow.pids2chrgs(pid)\n",
    "        if use_one_hot:\n",
    "            nodes = one_hot\n",
    "        else:\n",
    "            nodes = np.concatenate((mass, charge), axis=-1)\n",
    "            nodes = np.sign(nodes) * np.log(np.abs(nodes) + 1)\n",
    "        atom_mask = (pid[..., 0] != 0).astype(bool)\n",
    "        value['p4s'] = p4s\n",
    "        value['nodes'] = nodes\n",
    "        value['label'] = value['label']\n",
    "        value['atom_mask'] = atom_mask\n",
    "\n",
    "    datasets = {split: JetDataset(value['label'], value['p4s'], value['nodes'], value['atom_mask'], batch_size)\n",
    "                for split, value in data.items()}\n",
    "\n",
    "    dataloaders = {split: datasets[split].map() for split, dataset in datasets.items()}\n",
    "\n",
    "    return datasets, dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e70f8fd-3bd3-4072-b53d-68665713c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataloaders = retrieve_dataloaders(256, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa3169-db42-44ce-81d8-14a0fa076735",
   "metadata": {},
   "source": [
    "define the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01cef55-e4de-4389-807b-59d98f03e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LGEB(nn.Cell):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout=0., c_weight=1.0, last_layer=False):\n",
    "        super(LGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2  # dims for Minkowski norm & inner product\n",
    "\n",
    "        # Define the edge feature transformation network (phi_e)\n",
    "        self.phi_e = nn.SequentialCell([\n",
    "            nn.Dense(n_input * 2 + n_edge_attr, n_hidden, has_bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(n_hidden, n_hidden),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "\n",
    "        # Define the hidden state transformation network (phi_h)\n",
    "        self.phi_h = nn.SequentialCell([\n",
    "            nn.Dense(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(n_hidden, n_output)\n",
    "        ])\n",
    "\n",
    "        # Define the transformation network for x (phi_x)\n",
    "        layer = nn.Dense(n_hidden, 1, has_bias=False, weight_init=XavierUniform(gain=0.001))\n",
    "        self.phi_x = nn.SequentialCell([\n",
    "            nn.Dense(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer\n",
    "        ])\n",
    "\n",
    "        # Define the transformation network for m (phi_m)\n",
    "        self.phi_m = nn.SequentialCell([\n",
    "            nn.Dense(n_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "\n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            self.phi_x = None\n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = ops.Concat(axis=1)([hi, hj, norms, dots])\n",
    "        out = self.phi_e(out)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = ops.unsorted_segment_sum(m, i, num_segments=h.shape[0])\n",
    "        agg = ops.Concat(axis=1)([h, agg, node_attr])\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m):\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        trans = ops.clamp(trans, min=-100, max=100)\n",
    "        agg = ops.unsorted_segment_sum(trans, i, num_segments=x.shape[0])\n",
    "        x = x + agg * self.c_weight\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = ops.Sub()(x[i], x[j])\n",
    "        norms = self.normsq4(x_diff).view((-1, 1))\n",
    "        dots = self.dotsq4(x[i], x[j]).view((-1, 1))\n",
    "        norms, dots = self.psi(norms), self.psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def unsorted_segment_sum(self, data, segment_ids, num_segments):\n",
    "        result = Tensor([0])\n",
    "        result = result.new_zeros((num_segments, data.shape[1]))\n",
    "        result.index_add_(result, segment_ids, data)\n",
    "        return result\n",
    "\n",
    "    def unsorted_segment_mean(self, data, segment_ids, num_segments):\n",
    "        result = Tensor([0])\n",
    "        result = result.new_zeros((num_segments, data.shape[1]))\n",
    "        count = Tensor([0])\n",
    "        count = count.new_zeros((num_segments, data.shape[1]))\n",
    "        result.index_add_(result, segment_ids, data)\n",
    "        count.index_add_(count, segment_ids, Tensor.ones_like(data))\n",
    "        return result / ops.Minimum()(count, Tensor.ones_like(count))\n",
    "    \n",
    "    def normsq4(self, p):\n",
    "        psq = ops.Pow()(p, 2)\n",
    "        return 2 * psq[..., 0] - ops.ReduceSum()(psq, -1)\n",
    "\n",
    "    def dotsq4(self, p, q):\n",
    "        psq = ops.Mul()(p, q)\n",
    "        return 2 * psq[..., 0] - ops.ReduceSum()(psq, -1)\n",
    "    \n",
    "    def psi(self, p):\n",
    "        return ops.Sign()(p) * ops.Log()(ops.Abs()(p) + 1)\n",
    "    \n",
    "    def construct(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        m = self.m_model(h[i], h[j], norms, dots)  # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class LorentzNet(nn.Cell):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class=2, n_layers=6, c_weight=1e-3, dropout=0.1):\n",
    "        super(LorentzNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Dense(n_scalar, n_hidden, has_bias=True)\n",
    "        self.LGEBs = nn.CellList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i == n_layers - 1))\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.SequentialCell([\n",
    "            nn.Dense(self.n_hidden, self.n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=1-dropout),\n",
    "            nn.Dense(self.n_hidden, n_class)\n",
    "        ])\n",
    "\n",
    "    def construct(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "        #print(h.shape)\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n",
    "            #print(h.shape, x.shape)\n",
    "\n",
    "        h = ops.Mul()(h, node_mask)\n",
    "        #print(h.shape)\n",
    "        h = ops.Reshape()(h, (-1, n_nodes, self.n_hidden))\n",
    "        #print(h.shape)\n",
    "        h = ops.ReduceMean(keep_dims=False)(h, 1)\n",
    "        #print(h.shape)\n",
    "        pred = self.graph_dec(h)\n",
    "        #print(pred.shape)\n",
    "        return ops.squeeze(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b827a-8f6f-4fdb-8e03-70b328109ef3",
   "metadata": {},
   "source": [
    "initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5f25b5-f873-4348-a696-79cc879b6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LorentzNet(n_scalar = 8, n_hidden = 72, n_class = 2,\n",
    "                       dropout = 0.2, n_layers = 6,\n",
    "                       c_weight = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5af00-605c-4632-9832-a20a78cd811c",
   "metadata": {},
   "source": [
    "define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c4bf371-8f81-4284-93c9-a0dac4e593cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=0.001, weight_decay=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20994a11-c4ba-4493-9779-41b1a0c1ce2d",
   "metadata": {},
   "source": [
    "define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4badc26f-3383-4e0e-8142-5f01ee99e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(nodes, atom_positions, edges, atom_mask, edge_mask, n_nodes, label):\n",
    "    logits = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "def train_loop(model, dataloader):\n",
    "    num_batches = len(dataloader)\n",
    "    model.set_train()\n",
    "    st = 0\n",
    "    total, loss, correct = 0, 0, 0\n",
    "    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in enumerate(dataloader): \n",
    "        if i == 0:\n",
    "            st = time.time()\n",
    "        label = label.astype(ms.int32)\n",
    "        p4s = p4s.astype(ms.float32)\n",
    "        nodes = nodes.astype(ms.float32)\n",
    "        atom_mask = atom_mask.astype(ms.float32)\n",
    "        edge_mask = edge_mask.astype(ms.float32)\n",
    "        edges = edges.astype(ms.int32)\n",
    "        batch_size, n_nodes, _ = p4s.shape\n",
    "        atom_positions = p4s.reshape(batch_size * n_nodes, -1)\n",
    "        atom_mask = atom_mask.reshape(batch_size * n_nodes, -1)\n",
    "        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1)\n",
    "        nodes = nodes.reshape(batch_size * n_nodes, -1)\n",
    "        (_, logits), grads = grad_fn(nodes, atom_positions, edges, atom_mask, edge_mask, n_nodes, label)        \n",
    "        optimizer(grads)\n",
    "        loss += loss_fn(logits, label).asnumpy()\n",
    "        correct += (logits.argmax(1) == label).asnumpy().sum()\n",
    "        total += len(p4s)\n",
    "        print(f\"loss: {loss/(i+1):>7f} acc: {100*correct/total:>0.1f} [{i:>3d}/{num_batches:>3d}] time: [{time.time()-st:>0.1f}/{(time.time()-st)/(i+1)*num_batches:>0.1f}]\")\n",
    "\n",
    "def test_loop(model, dataloader, loss_fn):\n",
    "    num_batches =len(dataloader)\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in enumerate(dataloader): \n",
    "        label = label.astype(ms.int32)\n",
    "        p4s = p4s.astype(ms.float32)\n",
    "        nodes = nodes.astype(ms.float32)\n",
    "        atom_mask = atom_mask.astype(ms.float32)\n",
    "        edge_mask = edge_mask.astype(ms.float32)\n",
    "        edges = edges.astype(ms.int32)\n",
    "        batch_size, n_nodes, _ = p4s.shape\n",
    "        atom_positions = p4s.reshape(batch_size * n_nodes, -1)\n",
    "        atom_mask = atom_mask.reshape(batch_size * n_nodes, -1)\n",
    "        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1)\n",
    "        nodes = nodes.reshape(batch_size * n_nodes, -1)\n",
    "        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)\n",
    "        total += len(p4s)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Valid: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722789cb-afb3-469d-b81f-49e6b9367a62",
   "metadata": {},
   "source": [
    "start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce879e3e-e4bb-4cd7-a5bf-c296d142a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for t in range(10):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(model, dataloaders['train'])\n",
    "    print()\n",
    "    test_loop(model, dataloaders['val'], loss_fn)\n",
    "\n",
    "print('Test')\n",
    "test_loop(model, dataloaders['test'], loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a258083-2043-448b-ad23-5c5d7b11820c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dcc49d-51ae-42bf-9014-0d334d05b321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4dcd42-198d-4bc3-962c-1d368aea3aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cbf651-b0a5-4c23-b79f-48f6763eff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229a712-adca-41ee-9d90-86d7c35859ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24e93f-b082-4e87-96fa-d22ba97db850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6d7eb-1a0f-4c9a-84a9-5aa1da583780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946b44a-8982-4039-ad91-cffad0a4b85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2030",
   "language": "python",
   "name": "2030"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
